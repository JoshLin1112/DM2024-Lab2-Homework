{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: æž—å“²å…† (NTPU)\n",
    "\n",
    "Student ID: 711233103\n",
    "\n",
    "GitHub ID: JoshLin1112\n",
    "\n",
    "Kaggle name: josh4102\n",
    "\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data preparation   \n",
    "   \n",
    "2. Data pre-processing ( for Transformer model )   \n",
    "   \n",
    "3. Transformer model training\n",
    "    - 3.1 BERT\n",
    "\n",
    "    - 3.2 RoBERTa\n",
    "\n",
    "4. Transformer model prediction\n",
    "    - 4.1 BERT\n",
    "\n",
    "    - 4.2 RoBERTa\n",
    "5. Data pre-processing ( for fasttext, word2vec model )\n",
    "\n",
    "6. Fasttext model training and prediction\n",
    "\n",
    "7. Word2vec pre-trained model prediction with different classifier \n",
    "    - 8.1 Logistic Regression Classifier  \n",
    "       \n",
    "    - 8.2 XGBoost Classifier  \n",
    "\n",
    "8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part.1 - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>827</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['mixedfeeling', 'butim...</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>368</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x29d0...</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>498</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2a6a...</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x24fa...</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Sundayvibes'], 'tweet...</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score          _index  \\\n",
       "0           391  hashtag_tweets   \n",
       "1           433  hashtag_tweets   \n",
       "2           232  hashtag_tweets   \n",
       "3           376  hashtag_tweets   \n",
       "4           989  hashtag_tweets   \n",
       "...         ...             ...   \n",
       "1867530     827  hashtag_tweets   \n",
       "1867531     368  hashtag_tweets   \n",
       "1867532     498  hashtag_tweets   \n",
       "1867533     840  hashtag_tweets   \n",
       "1867534     360  hashtag_tweets   \n",
       "\n",
       "                                                   _source  \\\n",
       "0        {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1        {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2        {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3        {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4        {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "...                                                    ...   \n",
       "1867530  {'tweet': {'hashtags': ['mixedfeeling', 'butim...   \n",
       "1867531  {'tweet': {'hashtags': [], 'tweet_id': '0x29d0...   \n",
       "1867532  {'tweet': {'hashtags': [], 'tweet_id': '0x2a6a...   \n",
       "1867533  {'tweet': {'hashtags': [], 'tweet_id': '0x24fa...   \n",
       "1867534  {'tweet': {'hashtags': ['Sundayvibes'], 'tweet...   \n",
       "\n",
       "                  _crawldate   _type  \n",
       "0        2015-05-23 11:42:47  tweets  \n",
       "1        2016-01-28 04:52:09  tweets  \n",
       "2        2017-12-25 04:39:20  tweets  \n",
       "3        2016-01-24 23:53:05  tweets  \n",
       "4        2016-01-08 17:18:59  tweets  \n",
       "...                      ...     ...  \n",
       "1867530  2015-05-12 12:51:52  tweets  \n",
       "1867531  2017-10-02 17:54:04  tweets  \n",
       "1867532  2016-10-10 11:04:32  tweets  \n",
       "1867533  2016-09-02 14:25:06  tweets  \n",
       "1867534  2016-11-16 01:40:07  tweets  \n",
       "\n",
       "[1867535 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data\n",
    "tweets_raw = pd.read_json(\"kaggle-inclass/tweets_DM.json\", lines=True)\n",
    "identification = pd.read_csv('kaggle-inclass/data_identification.csv')\n",
    "emotion = pd.read_csv('kaggle-inclass/emotion.csv')\n",
    "tweets_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the values in '_source' column\n",
    "tweets_raw = tweets_raw.join(pd.json_normalize(tweets_raw['_source']))\n",
    "\n",
    "# drop unnecessary columns and rename columns\n",
    "tweets_raw = tweets_raw.drop(columns=['_source', '_index', '_score', '_crawldate', '_type']) \n",
    "tweets_raw = tweets_raw.rename(columns={'tweet.hashtags': 'hashtags', 'tweet.tweet_id': 'tweet_id', 'tweet.text': 'text'})\n",
    "tweets_raw = tweets_raw[['tweet_id', 'hashtags', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                       hashtags  \\\n",
       "0  0x376b20                     [Snapchat]   \n",
       "1  0x2d5350  [freepress, TrumpLegacy, CNN]   \n",
       "2  0x28b412                   [bibleverse]   \n",
       "3  0x1cd5b0                             []   \n",
       "4  0x2de201                             []   \n",
       "\n",
       "                                                text identification  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...          train   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...          train   \n",
       "2  Confident of your obedience, I write to you, k...           test   \n",
       "3                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>          train   \n",
       "4  \"Trust is not the same as faith. A friend is s...           test   \n",
       "\n",
       "        emotion  \n",
       "0  anticipation  \n",
       "1       sadness  \n",
       "2           NaN  \n",
       "3          fear  \n",
       "4           NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the identification and emotion datasets\n",
    "tweets_train_test = tweets_raw.merge(identification, how='left', on='tweet_id')\n",
    "tweets_df = tweets_train_test.merge(emotion, how='left', on='tweet_id')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot valus of emotion\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.countplot(x='emotion', data=tweets_df)\n",
    "plt.title('Distribution of Emotions')\n",
    "plt.show()\n",
    "\n",
    "# We can found that the distribution of emotions is not balanced\n",
    "# Texts belong to 'joy' is much more than other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('kaggle-inclass/df_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part.2 - Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet corpora are informal and often contain non-standard language, such as @-mentions for tagging users. These non-essential symbols should be removed during pre-processing to ensure accurate analysis. Other example of non-essential symbols:   \n",
    "1. URL\n",
    "2. #(Hashtag)\n",
    "3. repeat alphabets\n",
    "4. some unknown tokens in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# tweets_df = pd.read_csv('kaggle-inclass/df_tweets.csv')\n",
    "tweets_df = pd.read_csv('df_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # remove @\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # remove hashtag sign'#'\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # deal with the repeat alphabets , \"soooo happy\" -> \"so happy\"\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    # remove unknown term '<LH>, lh'\n",
    "    text = re.sub('<LH>','', text)\n",
    "    text = re.sub('lh','', text)\n",
    "    # lower the text and remoce space\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "tweets_df['text'] = tweets_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet corpora often contain emojis, which need to be ``demojified`` or replaced with their corresponding text to ensure accurate text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "from emoji_translate.emoji_translate import Translator\n",
    "\n",
    "# translator of emojis\n",
    "emo = Translator(exact_match_only=False, randomize=True)\n",
    "\n",
    "# bulid a dictionary of all emojis\n",
    "# note that we need to add a space before and after each emoji, prevent the situations like \"ðŸ‘ðŸ‘ðŸ‘\" -> \"thumbthumbthumb\"\n",
    "all_emojis = [char for char in emoji.EMOJI_DATA]\n",
    "replace_dict = {emoji_char: ' ' + emo.demojify(emoji_char).strip(':') + ' ' \n",
    "                for emoji_char in all_emojis}\n",
    "\n",
    "def demojify_unique(text):\n",
    "\n",
    "    # emoji mapping\n",
    "    for emoji_char, replacement in replace_dict.items():\n",
    "        text = text.replace(emoji_char, replacement)\n",
    "    # remove extra spaces\n",
    "    result = ' '.join(text.split())\n",
    "    # remove duplicate words , e.g. \"hello hello\" -> \"hello\"\n",
    "    result = re.sub(r'\\b(\\w+)( \\1)+\\b', r'\\1', result)\n",
    "    return result\n",
    "\n",
    "tweets_df['text'] = tweets_df['text'].apply(demojify_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i honestly have no luck picking coupons, ned to give up i think.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.sample(5)[['text', 'emotion']].iloc[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part. 3 Transformer model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the beginnig, I chose ``BERT`` as the training model for several reasons. First, ``BERT`` is a newer technique in Natural Language Processing compared to ``BOW`` or ``Word2vec``. This suggests it has the potential for superior performance on the task. Additionally, BERT's hardware requirements are still within the capabilities of my computer. Considering both factors, ``BERT`` appears to be the most suitable choice for my needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867535\n"
     ]
    }
   ],
   "source": [
    "tweets_train = tweets_df[tweets_df['identification'] == 'train']\n",
    "tweets_test = tweets_df[tweets_df['identification'] == 'test']\n",
    "print(len(tweets_train) + len(tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we'll train a BERT model or other NN models by Pytorch, we should define a ``Dataset`` class first to load our data and tokenize the texts. And I create two kind of dataset to try some diffirent input.  \n",
    "- TweetDataset treat both text and hashtag as input.\n",
    "- NoHashtagTweetDataset only use text as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model from huggingface\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=8)  \n",
    "\n",
    "# create dataset\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, hashtags, labels):\n",
    "        self.texts = texts\n",
    "        self.hashtags = hashtags\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # combine text and hashtag, and add [SEP] between them\n",
    "        text = self.texts[idx] + \" [SEP] \" + self.hashtags[idx]\n",
    "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
    "        \n",
    "        # covert to tensor\n",
    "        item = {key: torch.tensor(val) for key, val in inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# We need to encode the emotions to vector\n",
    "labels = [\"joy\", \"anticipation\", \"trust\", \"sadness\", \"disgust\", \"fear\", \"surprise\", \"anger\"] \n",
    "label_encoder = LabelEncoder()\n",
    "train_emotion_label = tweets_train['emotion'].tolist()\n",
    "train_encoded_labels = label_encoder.fit_transform(train_emotion_label)\n",
    "\n",
    "\n",
    "# extract data\n",
    "train_texts = tweets_train['text'].tolist()  # text list\n",
    "train_hashtags = tweets_train['hashtags'].tolist()  # hashtag list \n",
    "\n",
    "# create dataset\n",
    "train_dataset = TweetDataset(train_texts, train_hashtags, train_encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# create dataset\n",
    "class NoHashtagTweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # combine text and hashtag, and add [SEP] between them\n",
    "        text = self.texts[idx]\n",
    "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
    "        \n",
    "        # covert to tensor\n",
    "        item = {key: torch.tensor(val) for key, val in inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# extract data\n",
    "# labels = [\"joy\", \"anticipation\", \"trust\", \"sadness\", \"disgust\", \"fear\", \"surprise\", \"anger\"] \n",
    "# label_encoder = LabelEncoder()\n",
    "# train_emotion_label = tweets_train['emotion'].tolist()\n",
    "train_encoded_labels = label_encoder.fit_transform(train_emotion_label)\n",
    "train_texts_no_hashtag = tweets_train['text'].tolist()  # text list\n",
    "\n",
    "# create dataset\n",
    "train_dataset_no_hashtag = NoHashtagTweetDataset(train_texts_no_hashtag, train_encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I used trainer function in transformer package to train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34116 [00:00<?, ?it/s]c:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  1%|â–         | 500/34116 [02:34<2:52:59,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4631, 'grad_norm': 4.319309711456299, 'learning_rate': 1.970688240121937e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 1000/34116 [05:10<2:52:49,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2515, 'grad_norm': 3.081602096557617, 'learning_rate': 1.9413764802438738e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1500/34116 [07:44<2:40:56,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1809, 'grad_norm': 3.3749303817749023, 'learning_rate': 1.9120647203658108e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 2000/34116 [10:14<2:38:16,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1453, 'grad_norm': 2.86291241645813, 'learning_rate': 1.8827529604877478e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 2500/34116 [12:43<2:33:16,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.121, 'grad_norm': 3.6500117778778076, 'learning_rate': 1.8534412006096848e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 3000/34116 [15:11<2:31:03,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0978, 'grad_norm': 2.980592966079712, 'learning_rate': 1.8241294407316218e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 3500/34116 [17:39<2:28:52,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0847, 'grad_norm': 3.4000930786132812, 'learning_rate': 1.7948176808535588e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 4000/34116 [20:07<2:25:49,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0706, 'grad_norm': 2.997537136077881, 'learning_rate': 1.7655059209754954e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 4500/34116 [22:35<2:23:30,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0678, 'grad_norm': 3.2861599922180176, 'learning_rate': 1.7361941610974324e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 5000/34116 [25:03<2:21:27,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0552, 'grad_norm': 2.613903760910034, 'learning_rate': 1.7068824012193694e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 5500/34116 [27:32<2:19:01,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0515, 'grad_norm': 3.114243507385254, 'learning_rate': 1.6775706413413064e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 6000/34116 [30:00<2:15:23,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0348, 'grad_norm': 3.1524670124053955, 'learning_rate': 1.6482588814632434e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 6500/34116 [32:28<2:14:08,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0338, 'grad_norm': 3.2407796382904053, 'learning_rate': 1.6189471215851803e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 7000/34116 [34:55<2:11:27,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0284, 'grad_norm': 2.944417715072632, 'learning_rate': 1.589635361707117e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 7500/34116 [37:23<2:09:16,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0184, 'grad_norm': 3.1222901344299316, 'learning_rate': 1.560323601829054e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 8000/34116 [39:51<2:06:26,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0231, 'grad_norm': 3.2213597297668457, 'learning_rate': 1.531011841950991e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–       | 8500/34116 [42:19<2:03:50,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0182, 'grad_norm': 2.9828176498413086, 'learning_rate': 1.5017000820729278e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–‹       | 9000/34116 [44:48<2:01:38,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0072, 'grad_norm': 3.032158136367798, 'learning_rate': 1.4723883221948648e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 9500/34116 [47:16<1:59:29,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0196, 'grad_norm': 3.137104034423828, 'learning_rate': 1.4430765623168018e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 10000/34116 [49:44<1:57:22,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0064, 'grad_norm': 2.4165000915527344, 'learning_rate': 1.4137648024387384e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 10500/34116 [52:12<1:54:15,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0056, 'grad_norm': 3.161746025085449, 'learning_rate': 1.3844530425606754e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 11000/34116 [54:40<1:51:52,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9996, 'grad_norm': 3.2385518550872803, 'learning_rate': 1.3551412826826124e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 11500/34116 [57:08<1:49:53,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9777, 'grad_norm': 3.895641803741455, 'learning_rate': 1.3258295228045492e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 12000/34116 [59:36<1:47:47,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.938, 'grad_norm': 3.1612257957458496, 'learning_rate': 1.2965177629264862e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 12500/34116 [1:02:04<1:44:52,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9322, 'grad_norm': 3.726457118988037, 'learning_rate': 1.2672060030484232e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13000/34116 [1:04:32<1:42:26,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9291, 'grad_norm': 3.7427072525024414, 'learning_rate': 1.23789424317036e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 13500/34116 [1:07:00<1:40:07,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9238, 'grad_norm': 3.2307181358337402, 'learning_rate': 1.208582483292297e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14000/34116 [1:09:28<1:37:49,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9295, 'grad_norm': 3.8505146503448486, 'learning_rate': 1.179270723414234e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14500/34116 [1:11:56<1:35:05,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9288, 'grad_norm': 3.9039008617401123, 'learning_rate': 1.1499589635361708e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15000/34116 [1:14:24<1:32:27,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9226, 'grad_norm': 3.7744052410125732, 'learning_rate': 1.1206472036581078e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15500/34116 [1:16:52<1:29:50,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9216, 'grad_norm': 3.6964969635009766, 'learning_rate': 1.0913354437800448e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16000/34116 [1:19:20<1:27:56,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9312, 'grad_norm': 3.495168685913086, 'learning_rate': 1.0620236839019815e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 16500/34116 [1:21:48<1:25:46,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9257, 'grad_norm': 3.559839963912964, 'learning_rate': 1.0327119240239184e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17000/34116 [1:24:16<1:23:09,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9245, 'grad_norm': 4.075736999511719, 'learning_rate': 1.0034001641458554e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17500/34116 [1:26:44<1:20:58,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.926, 'grad_norm': 3.6889140605926514, 'learning_rate': 9.740884042677923e-06, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 18000/34116 [1:29:12<1:18:28,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.921, 'grad_norm': 3.647735834121704, 'learning_rate': 9.447766443897292e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18500/34116 [1:31:40<1:15:16,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9218, 'grad_norm': 3.423539638519287, 'learning_rate': 9.154648845116662e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 19000/34116 [1:34:08<1:12:58,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9146, 'grad_norm': 3.6936895847320557, 'learning_rate': 8.86153124633603e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 19500/34116 [1:36:36<1:10:42,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9171, 'grad_norm': 3.586902618408203, 'learning_rate': 8.5684136475554e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 20000/34116 [1:39:04<1:08:38,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9155, 'grad_norm': 3.5693867206573486, 'learning_rate': 8.275296048774769e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20500/34116 [1:41:32<1:05:47,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9104, 'grad_norm': 3.402540683746338, 'learning_rate': 7.982178449994138e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 21000/34116 [1:44:00<1:03:48,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9168, 'grad_norm': 3.43103289604187, 'learning_rate': 7.689060851213508e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 21500/34116 [1:46:29<1:01:10,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9144, 'grad_norm': 3.0952653884887695, 'learning_rate': 7.395943252432877e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22000/34116 [1:48:57<58:53,  3.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9101, 'grad_norm': 3.350175380706787, 'learning_rate': 7.102825653652246e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 22500/34116 [1:51:25<56:26,  3.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9039, 'grad_norm': 3.4650533199310303, 'learning_rate': 6.8097080548716155e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 23000/34116 [1:53:53<54:03,  3.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8802, 'grad_norm': 3.7294678688049316, 'learning_rate': 6.5165904560909846e-06, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 23500/34116 [1:56:21<51:35,  3.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8451, 'grad_norm': 4.417520523071289, 'learning_rate': 6.223472857310353e-06, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 24000/34116 [1:58:49<49:00,  3.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8533, 'grad_norm': 4.671696186065674, 'learning_rate': 5.930355258529723e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 24500/34116 [2:01:17<46:28,  3.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8485, 'grad_norm': 3.62711763381958, 'learning_rate': 5.637237659749092e-06, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25000/34116 [2:03:45<43:58,  3.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8512, 'grad_norm': 4.3262529373168945, 'learning_rate': 5.344120060968461e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25500/34116 [2:06:13<41:51,  3.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8415, 'grad_norm': 4.0846428871154785, 'learning_rate': 5.051002462187831e-06, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 26000/34116 [2:08:42<39:16,  3.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8507, 'grad_norm': 4.92640495300293, 'learning_rate': 4.7578848634072e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 26500/34116 [2:11:10<37:00,  3.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8437, 'grad_norm': 4.8902363777160645, 'learning_rate': 4.464767264626568e-06, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 27000/34116 [2:13:38<34:25,  3.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8449, 'grad_norm': 3.9212260246276855, 'learning_rate': 4.171649665845938e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 27500/34116 [2:16:06<32:05,  3.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8401, 'grad_norm': 4.611080646514893, 'learning_rate': 3.878532067065307e-06, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 28000/34116 [2:18:34<29:48,  3.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8465, 'grad_norm': 4.388956546783447, 'learning_rate': 3.585414468284676e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 28500/34116 [2:21:02<27:10,  3.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8407, 'grad_norm': 4.133814334869385, 'learning_rate': 3.2922968695040454e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 29000/34116 [2:23:30<24:46,  3.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8452, 'grad_norm': 4.945288181304932, 'learning_rate': 2.999179270723415e-06, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 29500/34116 [2:25:58<22:24,  3.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8421, 'grad_norm': 3.6948485374450684, 'learning_rate': 2.7060616719427835e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30000/34116 [2:28:26<20:01,  3.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8391, 'grad_norm': 3.3722355365753174, 'learning_rate': 2.412944073162153e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 30500/34116 [2:31:06<17:47,  3.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8413, 'grad_norm': 4.373429298400879, 'learning_rate': 2.119826474381522e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31000/34116 [2:36:24<58:20,  1.12s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.843, 'grad_norm': 3.8665316104888916, 'learning_rate': 1.8267088756008912e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 31500/34116 [2:42:43<12:44,  3.42it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8383, 'grad_norm': 4.119380950927734, 'learning_rate': 1.5335912768202605e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32000/34116 [2:45:31<10:17,  3.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8371, 'grad_norm': 4.369431972503662, 'learning_rate': 1.2404736780396295e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 32500/34116 [2:48:01<07:51,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8426, 'grad_norm': 3.8633251190185547, 'learning_rate': 9.473560792589988e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33000/34116 [2:50:31<05:23,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8296, 'grad_norm': 3.7817623615264893, 'learning_rate': 6.542384804783681e-07, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 33500/34116 [2:53:02<02:59,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.843, 'grad_norm': 3.8547914028167725, 'learning_rate': 3.611208816977372e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34000/34116 [2:55:31<00:33,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8318, 'grad_norm': 3.793322801589966, 'learning_rate': 6.800328291710635e-08, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34116/34116 [2:56:10<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10570.6563, 'train_samples_per_second': 413.095, 'train_steps_per_second': 3.227, 'train_loss': 0.9475786495577732, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34116, training_loss=0.9475786495577732, metrics={'train_runtime': 10570.6563, 'train_samples_per_second': 413.095, 'train_steps_per_second': 3.227, 'total_flos': 2.872465113942651e+17, 'train_loss': 0.9475786495577732, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"BERT-CLASSIFICATION-Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the first model, I used another dataset without hashtag as input to train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 2660/34116 [17:08<3:22:41,  2.59it/s]\n",
      "  1%|â–         | 500/34116 [02:31<2:50:27,  3.29it/s]\n",
      "  1%|â–         | 500/34116 [02:31<2:50:27,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7855, 'grad_norm': 6.4399847984313965, 'learning_rate': 1.970688240121937e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 1000/34116 [05:04<2:43:40,  3.37it/s]\n",
      "  3%|â–Ž         | 1000/34116 [05:05<2:43:40,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9334, 'grad_norm': 4.942432880401611, 'learning_rate': 1.9413764802438738e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1500/34116 [07:37<2:43:27,  3.33it/s] \n",
      "  4%|â–         | 1500/34116 [07:37<2:43:27,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8841, 'grad_norm': 6.257810592651367, 'learning_rate': 1.9120647203658108e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 2000/34116 [10:10<2:42:19,  3.30it/s]\n",
      "  6%|â–Œ         | 2000/34116 [10:10<2:42:19,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8642, 'grad_norm': 6.573243141174316, 'learning_rate': 1.8827529604877478e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 2500/34116 [12:43<2:38:10,  3.33it/s]\n",
      "  7%|â–‹         | 2500/34116 [12:43<2:38:10,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8515, 'grad_norm': 6.118330001831055, 'learning_rate': 1.8534412006096848e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 3000/34116 [15:16<2:35:35,  3.33it/s]\n",
      "  9%|â–‰         | 3000/34116 [15:16<2:35:35,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0282, 'grad_norm': 3.792872905731201, 'learning_rate': 1.8241294407316218e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 3500/34116 [17:49<2:35:30,  3.28it/s]\n",
      " 10%|â–ˆ         | 3500/34116 [17:49<2:35:30,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0894, 'grad_norm': 3.6624865531921387, 'learning_rate': 1.7948176808535588e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 4000/34116 [20:22<2:30:16,  3.34it/s]\n",
      " 12%|â–ˆâ–        | 4000/34116 [20:22<2:30:16,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.077, 'grad_norm': 3.308387041091919, 'learning_rate': 1.7655059209754954e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 4500/34116 [22:55<2:28:42,  3.32it/s]\n",
      " 13%|â–ˆâ–Ž        | 4500/34116 [22:55<2:28:42,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0742, 'grad_norm': 3.4816367626190186, 'learning_rate': 1.7361941610974324e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 5000/34116 [25:28<2:26:13,  3.32it/s]\n",
      " 15%|â–ˆâ–        | 5000/34116 [25:28<2:26:13,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.063, 'grad_norm': 3.5371110439300537, 'learning_rate': 1.7068824012193694e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 5500/34116 [28:01<2:23:36,  3.32it/s]\n",
      " 16%|â–ˆâ–Œ        | 5500/34116 [28:01<2:23:36,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0589, 'grad_norm': 3.4394211769104004, 'learning_rate': 1.6775706413413064e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 6000/34116 [30:33<2:20:08,  3.34it/s]\n",
      " 18%|â–ˆâ–Š        | 6000/34116 [30:33<2:20:08,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0439, 'grad_norm': 4.212697982788086, 'learning_rate': 1.6482588814632434e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 6500/34116 [33:07<2:19:38,  3.30it/s]\n",
      " 19%|â–ˆâ–‰        | 6500/34116 [33:07<2:19:38,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0464, 'grad_norm': 3.9892005920410156, 'learning_rate': 1.6189471215851803e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 7000/34116 [35:40<2:15:23,  3.34it/s]\n",
      " 21%|â–ˆâ–ˆ        | 7000/34116 [35:40<2:15:23,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0389, 'grad_norm': 4.085366249084473, 'learning_rate': 1.589635361707117e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 7500/34116 [38:11<2:08:19,  3.46it/s]\n",
      " 22%|â–ˆâ–ˆâ–       | 7500/34116 [38:11<2:08:19,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0309, 'grad_norm': 3.3676834106445312, 'learning_rate': 1.560323601829054e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 8000/34116 [40:36<2:03:37,  3.52it/s]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 8000/34116 [40:37<2:03:37,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0351, 'grad_norm': 3.65673828125, 'learning_rate': 1.531011841950991e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–       | 8500/34116 [43:02<2:02:15,  3.49it/s]\n",
      " 25%|â–ˆâ–ˆâ–       | 8500/34116 [43:02<2:02:15,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0305, 'grad_norm': 3.0427403450012207, 'learning_rate': 1.5017000820729278e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–‹       | 9000/34116 [45:28<2:00:20,  3.48it/s]\n",
      " 26%|â–ˆâ–ˆâ–‹       | 9000/34116 [45:28<2:00:20,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0202, 'grad_norm': 3.3257265090942383, 'learning_rate': 1.4723883221948648e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 9500/34116 [47:54<1:57:14,  3.50it/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 9500/34116 [47:54<1:57:14,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0328, 'grad_norm': 3.341809034347534, 'learning_rate': 1.4430765623168018e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 10000/34116 [50:20<1:54:32,  3.51it/s]\n",
      " 29%|â–ˆâ–ˆâ–‰       | 10000/34116 [50:20<1:54:32,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0181, 'grad_norm': 2.562134027481079, 'learning_rate': 1.4137648024387384e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 10500/34116 [52:46<1:52:05,  3.51it/s]\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 10500/34116 [52:46<1:52:05,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0196, 'grad_norm': 3.500394344329834, 'learning_rate': 1.3844530425606754e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 11000/34116 [55:12<1:50:32,  3.49it/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 11000/34116 [55:12<1:50:32,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0133, 'grad_norm': 3.3829073905944824, 'learning_rate': 1.3551412826826124e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 11500/34116 [57:38<1:47:39,  3.50it/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 11500/34116 [57:38<1:47:39,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9891, 'grad_norm': 3.997837543487549, 'learning_rate': 1.3258295228045492e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 12000/34116 [1:00:04<1:45:31,  3.49it/s]\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 12000/34116 [1:00:04<1:45:31,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.939, 'grad_norm': 3.8433074951171875, 'learning_rate': 1.2965177629264862e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 12500/34116 [1:02:30<1:43:32,  3.48it/s]\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 12500/34116 [1:02:30<1:43:32,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9315, 'grad_norm': 4.009241580963135, 'learning_rate': 1.2672060030484232e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13000/34116 [1:04:56<1:40:53,  3.49it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13000/34116 [1:04:56<1:40:53,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9313, 'grad_norm': 4.184355735778809, 'learning_rate': 1.23789424317036e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 13500/34116 [1:07:22<1:38:05,  3.50it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 13500/34116 [1:07:22<1:38:05,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9284, 'grad_norm': 3.4923651218414307, 'learning_rate': 1.208582483292297e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14000/34116 [1:09:47<1:36:25,  3.48it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14000/34116 [1:09:47<1:36:25,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9316, 'grad_norm': 4.084628105163574, 'learning_rate': 1.179270723414234e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14500/34116 [1:12:13<1:33:38,  3.49it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14500/34116 [1:12:13<1:33:38,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9341, 'grad_norm': 3.8909897804260254, 'learning_rate': 1.1499589635361708e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15000/34116 [1:14:40<1:31:22,  3.49it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15000/34116 [1:14:40<1:31:22,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9259, 'grad_norm': 3.8430445194244385, 'learning_rate': 1.1206472036581078e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15500/34116 [1:17:06<1:29:21,  3.47it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15500/34116 [1:17:06<1:29:21,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9235, 'grad_norm': 4.655088901519775, 'learning_rate': 1.0913354437800448e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16000/34116 [1:19:32<1:26:16,  3.50it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16000/34116 [1:19:32<1:26:16,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9306, 'grad_norm': 4.095736503601074, 'learning_rate': 1.0620236839019815e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 16500/34116 [1:21:58<1:23:12,  3.53it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 16500/34116 [1:21:58<1:23:12,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9286, 'grad_norm': 3.7406704425811768, 'learning_rate': 1.0327119240239184e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17000/34116 [1:24:21<1:19:46,  3.58it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17000/34116 [1:24:21<1:19:46,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9298, 'grad_norm': 3.64382004737854, 'learning_rate': 1.0034001641458554e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17500/34116 [1:26:44<1:17:58,  3.55it/s]\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17500/34116 [1:26:44<1:17:58,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9316, 'grad_norm': 4.553027153015137, 'learning_rate': 9.740884042677923e-06, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 18000/34116 [1:29:07<1:15:09,  3.57it/s]\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 18000/34116 [1:29:07<1:15:09,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9258, 'grad_norm': 3.7379097938537598, 'learning_rate': 9.447766443897292e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18500/34116 [1:31:30<1:13:22,  3.55it/s]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18500/34116 [1:31:30<1:13:22,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9278, 'grad_norm': 3.8447835445404053, 'learning_rate': 9.154648845116662e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 19000/34116 [1:33:54<1:11:07,  3.54it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 19000/34116 [1:33:54<1:11:07,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9205, 'grad_norm': 3.9960837364196777, 'learning_rate': 8.86153124633603e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 19500/34116 [1:36:17<1:08:13,  3.57it/s]\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 19500/34116 [1:36:17<1:08:13,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9201, 'grad_norm': 3.91886830329895, 'learning_rate': 8.5684136475554e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 20000/34116 [1:38:40<1:05:59,  3.57it/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 20000/34116 [1:38:40<1:05:59,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9221, 'grad_norm': 4.088264465332031, 'learning_rate': 8.275296048774769e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20500/34116 [1:41:04<1:03:41,  3.56it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20500/34116 [1:41:04<1:03:41,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9146, 'grad_norm': 3.6099042892456055, 'learning_rate': 7.982178449994138e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 21000/34116 [1:43:28<1:02:10,  3.52it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 21000/34116 [1:43:28<1:02:10,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9226, 'grad_norm': 3.6926259994506836, 'learning_rate': 7.689060851213508e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 21500/34116 [1:45:51<59:05,  3.56it/s]  \n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 21500/34116 [1:45:51<59:05,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9191, 'grad_norm': 3.260099172592163, 'learning_rate': 7.395943252432877e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22000/34116 [1:48:15<56:42,  3.56it/s]  \n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22000/34116 [1:48:15<56:42,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9171, 'grad_norm': 4.048596382141113, 'learning_rate': 7.102825653652246e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 22500/34116 [1:50:38<54:23,  3.56it/s]  \n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 22500/34116 [1:50:38<54:23,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9114, 'grad_norm': 3.4386885166168213, 'learning_rate': 6.8097080548716155e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 23000/34116 [1:53:06<52:38,  3.52it/s]  \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 23000/34116 [1:53:06<52:38,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8859, 'grad_norm': 4.199467182159424, 'learning_rate': 6.5165904560909846e-06, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 23500/34116 [1:55:34<50:49,  3.48it/s]  \n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 23500/34116 [1:55:34<50:49,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8511, 'grad_norm': 4.877624988555908, 'learning_rate': 6.223472857310353e-06, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 24000/34116 [1:58:04<48:47,  3.46it/s]  \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 24000/34116 [1:58:04<48:47,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8574, 'grad_norm': 4.884327411651611, 'learning_rate': 5.930355258529723e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 24500/34116 [2:00:38<47:23,  3.38it/s]  \n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 24500/34116 [2:00:38<47:23,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8531, 'grad_norm': 4.527825832366943, 'learning_rate': 5.637237659749092e-06, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25000/34116 [2:03:10<45:45,  3.32it/s]  \n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25000/34116 [2:03:10<45:45,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8567, 'grad_norm': 4.277301788330078, 'learning_rate': 5.344120060968461e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25500/34116 [2:05:40<41:09,  3.49it/s]  \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25500/34116 [2:05:40<41:09,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8442, 'grad_norm': 4.941051959991455, 'learning_rate': 5.051002462187831e-06, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 26000/34116 [2:08:05<37:58,  3.56it/s]  \n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 26000/34116 [2:08:05<37:58,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8576, 'grad_norm': 5.377959251403809, 'learning_rate': 4.7578848634072e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 26500/34116 [2:10:31<36:15,  3.50it/s]  \n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 26500/34116 [2:10:31<36:15,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8474, 'grad_norm': 5.1639084815979, 'learning_rate': 4.464767264626568e-06, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 27000/34116 [2:13:01<35:15,  3.36it/s]  \n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 27000/34116 [2:13:01<35:15,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8495, 'grad_norm': 4.182025909423828, 'learning_rate': 4.171649665845938e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 27500/34116 [2:15:33<32:46,  3.36it/s]  \n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 27500/34116 [2:15:33<32:46,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8455, 'grad_norm': 4.964960098266602, 'learning_rate': 3.878532067065307e-06, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 28000/34116 [2:18:06<28:44,  3.55it/s]  \n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 28000/34116 [2:18:06<28:44,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8512, 'grad_norm': 4.7054901123046875, 'learning_rate': 3.585414468284676e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 28500/34116 [2:20:31<26:05,  3.59it/s]  \n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 28500/34116 [2:20:31<26:05,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8451, 'grad_norm': 4.394354820251465, 'learning_rate': 3.2922968695040454e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 29000/34116 [2:22:59<24:28,  3.48it/s]  \n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 29000/34116 [2:22:59<24:28,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.849, 'grad_norm': 5.470950603485107, 'learning_rate': 2.999179270723415e-06, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 29500/34116 [2:25:29<21:40,  3.55it/s]  \n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 29500/34116 [2:25:29<21:40,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.847, 'grad_norm': 4.090369701385498, 'learning_rate': 2.7060616719427835e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30000/34116 [2:27:53<19:15,  3.56it/s]  \n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30000/34116 [2:27:53<19:15,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8418, 'grad_norm': 4.163991451263428, 'learning_rate': 2.412944073162153e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 30500/34116 [2:30:19<17:09,  3.51it/s]  \n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 30500/34116 [2:30:19<17:09,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8479, 'grad_norm': 5.083815097808838, 'learning_rate': 2.119826474381522e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31000/34116 [2:32:50<15:22,  3.38it/s]  \n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31000/34116 [2:32:50<15:22,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8508, 'grad_norm': 4.497685432434082, 'learning_rate': 1.8267088756008912e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 31500/34116 [2:35:18<12:17,  3.54it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 31500/34116 [2:35:18<12:17,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8411, 'grad_norm': 4.281408786773682, 'learning_rate': 1.5335912768202605e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32000/34116 [2:37:45<10:02,  3.51it/s]  \n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32000/34116 [2:37:45<10:02,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8423, 'grad_norm': 4.825133323669434, 'learning_rate': 1.2404736780396295e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 32500/34116 [2:40:12<07:33,  3.56it/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 32500/34116 [2:40:12<07:33,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8482, 'grad_norm': 3.917937755584717, 'learning_rate': 9.473560792589988e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33000/34116 [2:42:37<05:13,  3.56it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33000/34116 [2:42:37<05:13,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8365, 'grad_norm': 3.99609637260437, 'learning_rate': 6.542384804783681e-07, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 33500/34116 [2:45:02<02:52,  3.57it/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 33500/34116 [2:45:02<02:52,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8505, 'grad_norm': 4.53846549987793, 'learning_rate': 3.611208816977372e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34000/34116 [2:47:27<00:32,  3.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34000/34116 [2:47:27<00:32,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8367, 'grad_norm': 3.9402239322662354, 'learning_rate': 6.800328291710635e-08, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34116/34116 [2:48:02<00:00,  3.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34116/34116 [2:48:05<00:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10085.6062, 'train_samples_per_second': 432.962, 'train_steps_per_second': 3.383, 'train_loss': 0.925203125898102, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34116, training_loss=0.925203125898102, metrics={'train_runtime': 10085.6062, 'train_samples_per_second': 432.962, 'train_steps_per_second': 3.383, 'total_flos': 2.872465113942651e+17, 'train_loss': 0.925203125898102, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer_no_hashtag = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_no_hashtag,\n",
    ")\n",
    "\n",
    "trainer_no_hashtag.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_no_hashtag.save_model(\"NoHashtag-BERT-CLASSIFICATION-Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa is known as its great performance in classification task, so I want to know whether it has a significant difference compared to BERT.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# å‰µå»º Dataset\n",
    "class RoBERTaEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item\n",
    "\n",
    "labels = [\"joy\", \"anticipation\", \"trust\", \"sadness\", \"disgust\", \"fear\", \"surprise\", \"anger\"] \n",
    "label_encoder = LabelEncoder()\n",
    "train_emotion_label = tweets_train['emotion'].tolist()\n",
    "train_encoded_labels = label_encoder.fit_transform(train_emotion_label)\n",
    "train_texts_no_hashtag = tweets_train['text'].tolist() \n",
    "\n",
    "train_dataset_RoBERTa = RoBERTaEmotionDataset(train_texts_no_hashtag, train_encoded_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  1%|â–         | 500/34116 [02:44<3:08:17,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4496, 'grad_norm': 8.512863159179688, 'learning_rate': 1.970688240121937e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 1000/34116 [05:38<3:02:58,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2508, 'grad_norm': 6.697759628295898, 'learning_rate': 1.9413764802438738e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1500/34116 [08:28<3:00:40,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1839, 'grad_norm': 7.309689044952393, 'learning_rate': 1.9120647203658108e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 2000/34116 [11:11<2:47:02,  3.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.152, 'grad_norm': 8.014769554138184, 'learning_rate': 1.8827529604877478e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 2500/34116 [13:51<2:44:40,  3.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1275, 'grad_norm': 9.043797492980957, 'learning_rate': 1.8534412006096848e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 3000/34116 [16:28<2:38:19,  3.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1085, 'grad_norm': 6.245098114013672, 'learning_rate': 1.8241294407316218e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 3500/34116 [19:05<2:36:00,  3.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0958, 'grad_norm': 6.400434494018555, 'learning_rate': 1.7948176808535588e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 4000/34116 [21:42<2:34:00,  3.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0825, 'grad_norm': 6.27146577835083, 'learning_rate': 1.7655059209754954e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 4500/34116 [24:23<2:34:49,  3.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0796, 'grad_norm': 6.564360618591309, 'learning_rate': 1.7361941610974324e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 5000/34116 [27:03<2:31:23,  3.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0667, 'grad_norm': 5.863229751586914, 'learning_rate': 1.7068824012193694e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 5500/34116 [29:41<2:26:09,  3.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0656, 'grad_norm': 5.91044282913208, 'learning_rate': 1.6775706413413064e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 6000/34116 [32:21<2:25:14,  3.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0473, 'grad_norm': 6.131190299987793, 'learning_rate': 1.6482588814632434e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 6500/34116 [35:01<2:23:46,  3.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0462, 'grad_norm': 6.997139930725098, 'learning_rate': 1.6189471215851803e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 7000/34116 [37:39<2:18:52,  3.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0411, 'grad_norm': 5.879697799682617, 'learning_rate': 1.589635361707117e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 7500/34116 [40:16<2:15:32,  3.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.031, 'grad_norm': 5.554619312286377, 'learning_rate': 1.560323601829054e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 8000/34116 [42:52<2:12:36,  3.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0355, 'grad_norm': 6.009272575378418, 'learning_rate': 1.531011841950991e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–       | 8500/34116 [45:29<2:10:07,  3.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0306, 'grad_norm': 4.646533012390137, 'learning_rate': 1.5017000820729278e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–‹       | 9000/34116 [48:07<2:07:39,  3.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0194, 'grad_norm': 6.425856113433838, 'learning_rate': 1.4723883221948648e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 9500/34116 [50:44<2:05:35,  3.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0307, 'grad_norm': 6.432145118713379, 'learning_rate': 1.4430765623168018e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 10000/34116 [53:21<2:03:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0168, 'grad_norm': 4.692997455596924, 'learning_rate': 1.4137648024387384e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 10500/34116 [55:59<2:10:43,  3.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0176, 'grad_norm': 5.351382255554199, 'learning_rate': 1.3844530425606754e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 11000/34116 [58:39<2:00:05,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0093, 'grad_norm': 5.959741115570068, 'learning_rate': 1.3551412826826124e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 11500/34116 [1:01:19<1:55:30,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9917, 'grad_norm': 6.334312438964844, 'learning_rate': 1.3258295228045492e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 12000/34116 [1:03:56<1:52:28,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9693, 'grad_norm': 6.135651588439941, 'learning_rate': 1.2965177629264862e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 12500/34116 [1:06:33<1:50:27,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.965, 'grad_norm': 6.229388236999512, 'learning_rate': 1.2672060030484232e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 13000/34116 [1:09:11<1:47:44,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.957, 'grad_norm': 6.362489223480225, 'learning_rate': 1.23789424317036e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 13500/34116 [1:11:48<1:44:58,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9545, 'grad_norm': 6.581887722015381, 'learning_rate': 1.208582483292297e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14000/34116 [1:14:25<1:42:05,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9589, 'grad_norm': 5.975691318511963, 'learning_rate': 1.179270723414234e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 14500/34116 [1:17:03<1:39:38,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.958, 'grad_norm': 6.79091215133667, 'learning_rate': 1.1499589635361708e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15000/34116 [1:19:40<1:37:37,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9497, 'grad_norm': 8.038378715515137, 'learning_rate': 1.1206472036581078e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15500/34116 [1:22:16<1:34:42,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.948, 'grad_norm': 6.936270236968994, 'learning_rate': 1.0913354437800448e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 16000/34116 [1:24:53<1:31:56,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9589, 'grad_norm': 5.913122177124023, 'learning_rate': 1.0620236839019815e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 16500/34116 [1:27:30<1:29:26,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9534, 'grad_norm': 6.171879768371582, 'learning_rate': 1.0327119240239184e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 17000/34116 [1:30:07<1:27:46,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9542, 'grad_norm': 6.130119800567627, 'learning_rate': 1.0034001641458554e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17500/34116 [1:32:49<1:26:13,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9562, 'grad_norm': 6.172586441040039, 'learning_rate': 9.740884042677923e-06, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 18000/34116 [1:35:30<1:23:35,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.948, 'grad_norm': 7.327147960662842, 'learning_rate': 9.447766443897292e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18500/34116 [1:38:08<1:19:45,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9466, 'grad_norm': 7.014835357666016, 'learning_rate': 9.154648845116662e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 19000/34116 [1:40:45<1:17:29,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9412, 'grad_norm': 6.593326091766357, 'learning_rate': 8.86153124633603e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 19500/34116 [1:43:23<1:14:24,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.942, 'grad_norm': 5.698760032653809, 'learning_rate': 8.5684136475554e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 20000/34116 [1:46:00<1:11:59,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9406, 'grad_norm': 5.976863384246826, 'learning_rate': 8.275296048774769e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20500/34116 [1:48:37<1:08:48,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9361, 'grad_norm': 6.218288898468018, 'learning_rate': 7.982178449994138e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 21000/34116 [1:51:15<1:06:46,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.941, 'grad_norm': 6.049225330352783, 'learning_rate': 7.689060851213508e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 21500/34116 [1:53:53<1:05:28,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9362, 'grad_norm': 6.032218933105469, 'learning_rate': 7.395943252432877e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22000/34116 [1:56:34<1:02:13,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9357, 'grad_norm': 6.226777076721191, 'learning_rate': 7.102825653652246e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 22500/34116 [2:01:08<59:11,  3.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9298, 'grad_norm': 6.291817665100098, 'learning_rate': 6.8097080548716155e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 23000/34116 [2:03:45<57:26,  3.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9142, 'grad_norm': 6.6881513595581055, 'learning_rate': 6.5165904560909846e-06, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 23500/34116 [2:06:22<54:29,  3.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8895, 'grad_norm': 7.586518287658691, 'learning_rate': 6.223472857310353e-06, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 24000/34116 [2:09:00<51:43,  3.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8966, 'grad_norm': 7.958432674407959, 'learning_rate': 5.930355258529723e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 24500/34116 [2:11:38<48:44,  3.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8922, 'grad_norm': 7.260921955108643, 'learning_rate': 5.637237659749092e-06, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 25000/34116 [2:14:15<46:29,  3.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8923, 'grad_norm': 6.6180100440979, 'learning_rate': 5.344120060968461e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25500/34116 [2:16:52<43:51,  3.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8808, 'grad_norm': 7.458126068115234, 'learning_rate': 5.051002462187831e-06, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 26000/34116 [2:19:30<45:16,  2.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8926, 'grad_norm': 7.884998321533203, 'learning_rate': 4.7578848634072e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 26500/34116 [2:22:16<39:20,  3.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8854, 'grad_norm': 8.509636878967285, 'learning_rate': 4.464767264626568e-06, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 27000/34116 [2:24:55<37:06,  3.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8912, 'grad_norm': 7.816690444946289, 'learning_rate': 4.171649665845938e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 27500/34116 [2:27:35<33:56,  3.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8805, 'grad_norm': 7.237516403198242, 'learning_rate': 3.878532067065307e-06, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 28000/34116 [2:30:12<30:17,  3.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8882, 'grad_norm': 8.21292495727539, 'learning_rate': 3.585414468284676e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 28500/34116 [2:32:47<27:58,  3.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8817, 'grad_norm': 7.434571266174316, 'learning_rate': 3.2922968695040454e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 29000/34116 [2:35:23<25:37,  3.33it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.885, 'grad_norm': 8.925827026367188, 'learning_rate': 2.999179270723415e-06, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 29500/34116 [2:37:58<23:03,  3.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8815, 'grad_norm': 5.805539131164551, 'learning_rate': 2.7060616719427835e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 30000/34116 [2:40:33<20:35,  3.33it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8791, 'grad_norm': 6.680179119110107, 'learning_rate': 2.412944073162153e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 30500/34116 [2:43:07<18:03,  3.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.878, 'grad_norm': 7.2923359870910645, 'learning_rate': 2.119826474381522e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31000/34116 [2:45:46<17:18,  3.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.883, 'grad_norm': 6.7999773025512695, 'learning_rate': 1.8267088756008912e-06, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 31500/34116 [2:48:35<14:19,  3.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8767, 'grad_norm': 7.536407947540283, 'learning_rate': 1.5335912768202605e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32000/34116 [2:51:23<11:36,  3.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8736, 'grad_norm': 7.70119571685791, 'learning_rate': 1.2404736780396295e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 32500/34116 [2:54:11<08:25,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8833, 'grad_norm': 6.376413345336914, 'learning_rate': 9.473560792589988e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 33000/34116 [2:56:51<05:44,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8721, 'grad_norm': 6.77821159362793, 'learning_rate': 6.542384804783681e-07, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 33500/34116 [2:59:33<03:10,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8814, 'grad_norm': 7.179211616516113, 'learning_rate': 3.611208816977372e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 34000/34116 [3:02:14<00:36,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8702, 'grad_norm': 5.916433334350586, 'learning_rate': 6.800328291710635e-08, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34116/34116 [3:03:01<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10981.0641, 'train_samples_per_second': 397.656, 'train_steps_per_second': 3.107, 'train_loss': 0.9733366148317247, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34116, training_loss=0.9733366148317247, metrics={'train_runtime': 10981.0641, 'train_samples_per_second': 397.656, 'train_steps_per_second': 3.107, 'total_flos': 2.872465113942651e+17, 'train_loss': 0.9733366148317247, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RoBERTa_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=8)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.02,\n",
    "    save_total_limit=5,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer_RoBERTa_model = Trainer(\n",
    "    model=RoBERTa_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_no_hashtag,\n",
    ")\n",
    "\n",
    "trainer_RoBERTa_model.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_RoBERTa_model.save_model(\"RoBERTa-CLASSIFICATION-Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several times I tried, it seemed that the accuracy cannot be higher if I don't change the text pre-processing and model structure.  \n",
    "Therefore, I applied another pre-processing method by ``normalizeTweet.py``, which is right by 'VinAIResearch'.  \n",
    "And I also defined a custom model structure for RoBERTa in order to get a higher accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867535\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from TweetNormalizer import normalizeTweet\n",
    "df_tweets = pd.read_csv('df_tweets.csv')\n",
    "df_tweets['text'] = df_tweets['text'].apply(normalizeTweet)\n",
    "\n",
    "tweets_train = tweets_df[tweets_df['identification'] == 'train']\n",
    "tweets_test = tweets_df[tweets_df['identification'] == 'test']\n",
    "print(len(tweets_train) + len(tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "labels = [\"joy\", \"anticipation\", \"trust\", \"sadness\", \"disgust\", \"fear\", \"surprise\", \"anger\"] \n",
    "label_encoder = LabelEncoder()\n",
    "train_emotion_label = tweets_train['emotion'].tolist()\n",
    "train_encoded_labels = label_encoder.fit_transform(train_emotion_label)\n",
    "train_texts_no_hashtag = tweets_train['text'].tolist() \n",
    "\n",
    "train_dataset_RoBERTa = RoBERTaEmotionDataset(train_texts_no_hashtag, train_encoded_labels, tokenizer)\n",
    "dl_train = DataLoader(train_dataset_RoBERTa, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import torch\n",
    "from transformers import RobertaModel\n",
    "\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(256, 8)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Step 1: Pass input through RobertaModel\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        pooler = hidden_state[:, 0]  # Extract CLS token (batch_size, hidden_size)\n",
    "        pooler = self.pre_classifier(pooler)  # Shape: (batch_size, 256)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)  # Shape: (batch_size, 8)\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RoBERTa_model = RobertaClass()\n",
    "RoBERTa_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/4]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22744/22744 [55:34<00:00,  6.82it/s, Loss=1.1546, Accuracy=58.57%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      " - Training Loss: 1.1546\n",
      " - Training Accuracy: 58.57%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [2/4]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22744/22744 [59:15<00:00,  6.40it/s, Loss=1.0217, Accuracy=63.37%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      " - Training Loss: 1.0217\n",
      " - Training Accuracy: 63.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [3/4]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22744/22744 [59:17<00:00,  6.39it/s, Loss=0.9551, Accuracy=65.77%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      " - Training Loss: 0.9551\n",
      " - Training Accuracy: 65.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [4/4]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22744/22744 [55:55<00:00,  6.78it/s, Loss=0.8983, Accuracy=67.77%] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      " - Training Loss: 0.8983\n",
      " - Training Accuracy: 67.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  RoBERTa_model.parameters(), lr=3e-5)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    RoBERTa_model.train()\n",
    "\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "    for step, batch in enumerate(pbar, start=1):\n",
    "        # take necessary input\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        targets = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = RoBERTa_model(ids, mask)\n",
    "        # calculate loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1) \n",
    "        n_correct += calculate_accuracy(preds, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{tr_loss / nb_tr_steps:.4f}\",\n",
    "            \"Accuracy\": f\"{(n_correct * 100) / nb_tr_examples:.2f}%\"\n",
    "        })\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    epoch_accu = (n_correct * 100) / nb_tr_examples\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\" - Training Loss: {epoch_loss:.4f}\")\n",
    "    print(f\" - Training Accuracy: {epoch_accu:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 4. BERT model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved our model in the previous part. Next, we are going to use this pre-trained model to make the prediction on test datatset. Same process as last part, we need to define a dataset class to load test data. The difference between training data and test data is that test set doesn't have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "model = BertForSequenceClassification.from_pretrained(\"BERT-CLASSIFICATION-Model\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25749/25749 [07:41<00:00, 55.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trust' 'trust' 'joy' 'joy' 'trust' 'surprise' 'fear' 'joy' 'joy'\n",
      " 'sadness']\n"
     ]
    }
   ],
   "source": [
    "# create test dataset\n",
    "class TestEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, hashtags, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.hashtags = hashtags\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx] + \" [SEP] \" + self.hashtags[idx]\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
    "        item = {key: torch.tensor(val) for key, val in inputs.items()}\n",
    "        return item\n",
    "\n",
    "test_texts = tweets_test['text'].tolist()  # text list\n",
    "test_hashtags = tweets_test['hashtags'].tolist()  # hashtag list\n",
    "test_dataset = TestEmotionDataset(test_texts, test_hashtags, tokenizer)\n",
    "\n",
    "# predict the emotion by the model we have trained\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "\n",
    "# convert vector labels to text labels \n",
    "pred_emotions = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "# see the first 10 results\n",
    "print(pred_emotions[:10])\n",
    "\n",
    "# save results\n",
    "bert_result = pd.DataFrame({'text': test_texts, 'hashtags': test_hashtags, 'predicted_emotion': pred_emotions})\n",
    "bert_result.to_csv('bert_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "bert_result['id'] = tweets_test['tweet_id'].tolist()\n",
    "submission = bert_result[['id', 'predicted_emotion']].rename(columns={'predicted_emotion': 'emotion'})\n",
    "submission.to_csv('submission_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``The accuracy of the BERT model is 52.97%``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# å‰µå»º Dataset\n",
    "class RoBERTaTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "        }\n",
    "        return item\n",
    "\n",
    "\n",
    "test_texts = tweets_test['text'].tolist()  # text list\n",
    "test_dataset = RoBERTaTestDataset(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51497/51497 [07:31<00:00, 113.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anticipation' 'trust' 'joy' 'joy' 'trust' 'surprise' 'joy' 'anger' 'joy'\n",
      " 'sadness']\n"
     ]
    }
   ],
   "source": [
    "# predict the emotion by the model we have trained\n",
    "predictions = trainer_RoBERTa_model.predict(test_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "\n",
    "# convert vector labels to text labels \n",
    "pred_emotions = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "# see the first 10 results\n",
    "print(pred_emotions[:10])\n",
    "\n",
    "# save results\n",
    "roberta_result = pd.DataFrame({'text': test_texts, 'predicted_emotion': pred_emotions})\n",
    "roberta_result['id'] = tweets_test['tweet_id'].tolist()\n",
    "submission = roberta_result[['id', 'predicted_emotion']].rename(columns={'predicted_emotion': 'emotion'})\n",
    "submission.to_csv('submission_rberta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``The accuracy of RoBERTa model is 53.77%`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl_test = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6438/6438 [06:42<00:00, 16.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "RoBERTa_model.eval()\n",
    "predictions = []\n",
    "pbar = tqdm(dl_test)\n",
    "pbar.set_description(\"Predicting on test set\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in pbar:\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = RoBERTa_model(ids, mask)\n",
    "        _, preds = torch.max(outputs, dim=1)  # ç²å–é æ¸¬é¡žåˆ¥\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "RoBERTa_submisiion = pd.DataFrame({\n",
    "    \"id\": tweets_test['tweet_id'].tolist(),\n",
    "    \"emotion\": predictions\n",
    "})\n",
    "RoBERTa_submisiion['emotion'] = label_encoder.inverse_transform(RoBERTa_submisiion['emotion'])\n",
    "RoBERTa_submisiion.to_csv(\"RoBERTa_submisiion.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy             163990\n",
       "sadness          90084\n",
       "disgust          54937\n",
       "anticipation     49495\n",
       "trust            28106\n",
       "fear             11577\n",
       "anger             8290\n",
       "surprise          5493\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RoBERTa_submisiion['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``The accuracy of custom RoBERTa model is 52.79%``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``note`` : The difference between with hashtag and without hashtag is small, so I didn't show the result seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part. 5 Data pre-processing (for fasttext, word2vec model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm going to explore older NLP techniques, to see whether they are really worse than transformer. However, we can't directly use the pre-processed data from the ``Transformer`` model. These traditional methods often require more extensive pre-processing, as they lack the powerful pre-training and advanced techniques like attention, token type IDs, and encoder-decoder architectures found in Transformer model. To achieve good performance with these models, **word pre-processing** is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('kaggle-inclass/df_tweets.csv')\n",
    "tweets_train = tweets_df[tweets_df['identification'] == 'train']\n",
    "tweets_test = tweets_df[tweets_df['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18408\\1769358316.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_train['text_with_hashtags'] = tweets_train['text'] + \" \" + tweets_train['hashtags']\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18408\\1769358316.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_test['text_with_hashtags'] = tweets_test['text'] + \" \" + tweets_test['hashtags']\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18408\\1769358316.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_train['fasttext_format'] = '__label__' + tweets_train['emotion']\n"
     ]
    }
   ],
   "source": [
    "# combined text and hashtag\n",
    "tweets_train['text_with_hashtags'] = tweets_train['text'] + \" \" + tweets_train['hashtags']\n",
    "tweets_test['text_with_hashtags'] = tweets_test['text'] + \" \" + tweets_test['hashtags']\n",
    "\n",
    "# convert emotion to fastText format, which is like '__label__emotion'\n",
    "tweets_train['fasttext_format'] = '__label__' + tweets_train['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# define preprocess function\n",
    "def word_preprocess(text):\n",
    "\n",
    "    # lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # lemmatization and remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens \n",
    "                           if word.lower() not in stopwords.words('english')]\n",
    "    # filter out non-alphabetic tokens and short tokens\n",
    "    filtered_tokens = [word for word in filtered_tokens if word.isalpha() and len(word) > 2]\n",
    "\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_pre_process(text):\n",
    "\n",
    "    text = clean_text(text)\n",
    "    text = demojify_unique(text)\n",
    "    text = word_preprocess(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas(desc=\"Processing in DataFrame\")\n",
    "# tweets_train['text_with_hashtags'] = tweets_train['text_with_hashtags'].progress_apply(whole_pre_process)\n",
    "# tweets_test['text_with_hashtags'] = tweets_test['text_with_hashtags'].progress_apply(whole_pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training set as txt\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for text, label in zip(tweets_train['text_with_hashtags'], tweets_train['fasttext_format']):\n",
    "        f.write(f\"{label} {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part. 6 - Fasttext model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``fasttext.train_supervised`` command can easily start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# training model, `lr` is learning rate, `epoch` is training times\n",
    "model = fasttext.train_supervised(input='train.txt', lr=0.5, epoch=25, wordNgrams=2)\n",
    "# save model\n",
    "model.save_model(\"emotion_classification.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``model.predict`` command can easily generate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   text_with_hashtags       emotion  \\\n",
      "2   Confident of your obedience, I write to you, k...       disgust   \n",
      "4   \"Trust is not the same as faith. A friend is s...           joy   \n",
      "9   When do you have enough ? When are you satisfi...           joy   \n",
      "30  God woke you up, now chase the day #GodsPlan #...  anticipation   \n",
      "33  In these tough times, who do YOU turn to as yo...       disgust   \n",
      "\n",
      "    confidence  \n",
      "2     0.882856  \n",
      "4     0.591940  \n",
      "9     0.649454  \n",
      "30    0.998186  \n",
      "33    0.603192  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18408\\3626032741.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_test['emotion'] = [label[0].replace('__label__', '') for label in predictions[0]]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18408\\3626032741.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_test['confidence'] = [prob[0] for prob in predictions[1]]\n"
     ]
    }
   ],
   "source": [
    "# predict emotion by fastText model\n",
    "texts = tweets_test['text_with_hashtags'].tolist()\n",
    "predictions = model.predict(texts)\n",
    "\n",
    "tweets_test['emotion'] = [label[0].replace('__label__', '') for label in predictions[0]]\n",
    "tweets_test['confidence'] = [prob[0] for prob in predictions[1]]\n",
    "\n",
    "print(tweets_test[['text_with_hashtags', 'emotion', 'confidence']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file of fastText\n",
    "submisssion_fasttext = tweets_test[['tweet_id','emotion']]\n",
    "submisssion_fasttext = submisssion_fasttext.rename(columns={'tweet_id': 'id'})\n",
    "submisssion_fasttext.to_csv('kaggle-inclass/submission_fasttext.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``The accuracy of fasttext is 31.8%`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after extensive pre-processing, the model's accuracy is poor. It seems FastText is optimized for speed, not accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 7. - Word2vec pre-trained model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll utilize a pre-trained Twitter model from the ``gensim`` library to predict emotions. Gensim offers three pre-trained Twitter models with varying embedding dimensions: 25, 50, and 100. For our experiment, we'll use the ``glove-twitter-50`` model to evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('kaggle-inclass/df_tweets.csv')\n",
    "tweets_train = tweets_df[tweets_df['identification'] == 'train']\n",
    "tweets_test = tweets_df[tweets_df['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16400\\3159018936.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_train['text_with_hashtags'] = tweets_train['text'] + \" \" + tweets_train['hashtags']\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16400\\3159018936.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_test['text_with_hashtags'] = tweets_test['text'] + \" \" + tweets_test['hashtags']\n",
      "Processing in DataFrame: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1455563/1455563 [41:34<00:00, 583.51it/s] \n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16400\\3159018936.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_train['text_with_hashtags'] = tweets_train['text_with_hashtags'].progress_apply(whole_pre_process)\n",
      "Processing in DataFrame: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411972/411972 [12:43<00:00, 539.35it/s]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16400\\3159018936.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_test['text_with_hashtags'] = tweets_test['text_with_hashtags'].progress_apply(whole_pre_process)\n"
     ]
    }
   ],
   "source": [
    "tweets_train['text_with_hashtags'] = tweets_train['text'] + \" \" + tweets_train['hashtags']\n",
    "tweets_test['text_with_hashtags'] = tweets_test['text'] + \" \" + tweets_test['hashtags']\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Processing in DataFrame\")\n",
    "tweets_train['text_with_hashtags'] = tweets_train['text_with_hashtags'].progress_apply(whole_pre_process)\n",
    "tweets_test['text_with_hashtags'] = tweets_test['text_with_hashtags'].progress_apply(whole_pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('birthday', 0.9478022456169128),\n",
       " ('day', 0.884091317653656),\n",
       " ('love', 0.8704721331596375),\n",
       " ('thank', 0.8623553514480591),\n",
       " ('wish', 0.8603640794754028)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## excute if you see `SSL: CERTIFICATE_VERIFY_FAILED` error\n",
    "import ssl\n",
    "import urllib.request\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "glove_twitter_50_model = api.load(\"glove-twitter-50\")\n",
    "print('load ok')\n",
    "\n",
    "glove_twitter_50_model.most_similar('happy', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2          confident obedience write knowing even ask phi...\n",
       "4          trust faith friend someone trust putting faith...\n",
       "9          enough satisfied goal really money materialism...\n",
       "30         god woke chase day godsplan godswork godsplan ...\n",
       "33                               tough time turn symbol hope\n",
       "                                 ...                        \n",
       "1867525    message heard beginning love one another john kjv\n",
       "1867529    lad hath five barley loaf two small fish among...\n",
       "1867530    buy last ticket remaining show sell mixedfeeli...\n",
       "1867531                     swear hard work gone pay one day\n",
       "1867532                      card left wasnt idea get parcel\n",
       "Name: text_with_hashtags, Length: 411972, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test['text_with_hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "w2v_model = glove_twitter_50_model\n",
    "\n",
    "# convert sentences to embeddings\n",
    "def text_to_vector(text, w2v_model):\n",
    "    words = text.split()\n",
    "    vectors = [w2v_model[word] for word in words if word in w2v_model]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(w2v_model.vector_size)\n",
    "\n",
    "# extract text from training set and test set\n",
    "train_texts = tweets_train['text_with_hashtags'].tolist()\n",
    "test_texts = tweets_test['text_with_hashtags'].tolist()\n",
    "\n",
    "# convert sentences to embeddings\n",
    "train_vectors = np.array([text_to_vector(text, w2v_model) for text in train_texts])\n",
    "test_vectors = np.array([text_to_vector(text, w2v_model) for text in test_texts])\n",
    "\n",
    "# encode emotion\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(tweets_train['emotion'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part. 7.1 - Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: confident obedience write knowing even ask philemon bibleverse bibleverse\n",
      "Predicted label: anticipation\n",
      "----------------------------------------\n",
      "Text: trust faith friend someone trust putting faith anyone mistake christopher hitchens\n",
      "Predicted label: anticipation\n",
      "----------------------------------------\n",
      "Text: enough satisfied goal really money materialism money possession materialism money possession\n",
      "Predicted label: anger\n",
      "----------------------------------------\n",
      "Text: god woke chase day godsplan godswork godsplan godswork\n",
      "Predicted label: anticipation\n",
      "----------------------------------------\n",
      "Text: tough time turn symbol hope\n",
      "Predicted label: fear\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# use Logistic Regression as classifier\n",
    "clf = LogisticRegression(max_iter=1000, multi_class='ovr', solver='lbfgs', class_weight='balanced')  # try to balance class weight\n",
    "clf.fit(train_vectors, train_labels)\n",
    "\n",
    "# predict\n",
    "y_pred = clf.predict(test_vectors)\n",
    "\n",
    "# show first 5 results of prediction\n",
    "for i, text in enumerate(test_texts[:5]): \n",
    "    predicted_label = label_encoder.inverse_transform([y_pred[i]])[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy             68691\n",
       "disgust         63134\n",
       "anticipation    60692\n",
       "anger           54396\n",
       "sadness         47529\n",
       "fear            43192\n",
       "trust           39792\n",
       "surprise        34546\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_LR_emotions = [label_encoder.inverse_transform([x])[0] for x in y_pred.tolist()]\n",
    "tweet_id = tweets_test['tweet_id'].tolist()\n",
    "w2v_LR_result = pd.DataFrame({'id': tweet_id, 'emotion': w2v_LR_emotions})\n",
    "w2v_LR_result['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_LR_result.to_csv('kaggle-inclass/submission_w2v_LR.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``The accuracy of Logistic Regreesion is 29.9%`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Part. 7.2 - XGboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: confident obedience write knowing even ask philemon bibleverse bibleverse\n",
      "Predicted label: anticipation\n",
      "----------------------------------------\n",
      "Text: trust faith friend someone trust putting faith anyone mistake christopher hitchens\n",
      "Predicted label: anticipation\n",
      "----------------------------------------\n",
      "Text: enough satisfied goal really money materialism money possession materialism money possession\n",
      "Predicted label: joy\n",
      "----------------------------------------\n",
      "Text: god woke chase day godsplan godswork godsplan godswork\n",
      "Predicted label: anticipation\n",
      "----------------------------------------\n",
      "Text: tough time turn symbol hope\n",
      "Predicted label: joy\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "XGBclf = XGBClassifier(\n",
    "    objective='multi:softmax',  \n",
    "    num_class=8,               \n",
    "    max_depth=6,               \n",
    "    learning_rate=0.1,        \n",
    "    n_estimators=100,         \n",
    "    random_state=42           \n",
    ")\n",
    "XGBclf.fit(train_vectors, train_labels)\n",
    "y_pred = XGBclf.predict(test_vectors)\n",
    "for i, text in enumerate(test_texts[:5]): \n",
    "    predicted_label = label_encoder.inverse_transform([y_pred[i]])[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy             263053\n",
       "sadness          55197\n",
       "disgust          44115\n",
       "anticipation     38529\n",
       "trust             7264\n",
       "fear              3555\n",
       "anger              200\n",
       "surprise            59\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_XGB_emotions = [label_encoder.inverse_transform([x])[0] for x in y_pred.tolist()]\n",
    "tweet_id = tweets_test['tweet_id'].tolist()\n",
    "\n",
    "w2v_XGB_result = pd.DataFrame({'id': tweet_id, 'emotion': w2v_XGB_emotions})\n",
    "w2v_XGB_result['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_XGB_result.to_csv('kaggle-inclass/submission_w2v_XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``The accuracy of XGBoost is 33%`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part. 8 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this competition, we demonstrated that transformer-based models outperform models built on Word2Vec or other older NLP approaches. Even without extensive preprocessing, the transformer-based model still outperformed the Word2Vec-based model.It also proved that proper training and fine-tuning have a significant impact on model performance. \n",
    "\n",
    "##### However, when considering runtime, transformer models are undeniably resource-intensive. Despite using a powerful RTX 4090 GPU, it still took an hour to complete a single epoch. In comparison, the Word2Vec model paired with a classifier only required five minutes to execute.\n",
    "\n",
    "##### These results highlight the need to balance speed and accuracy when selecting a model. While newer approaches generally deliver superior results, practical constraints like time and hardware resources often necessitate trade-offs. Striking this balance is crucial for real-world applications.\n",
    "\n",
    "##### Finally, NLP remains a field that demands both high-quality hardware and a significant investment of time for exploration and experimentation. I deeply respect the researchers and engineers who have contributed to this field, as their innovative methods have shaped modern NLP.\n",
    "\n",
    "##### I'm grateful for the opportunity to dive deeper into NLP during this competition. I'll look forward to continuing my learning journey in NLP field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
